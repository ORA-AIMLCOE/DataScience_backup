{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae4645c2-b59e-419a-b108-b8b372acd1c6",
   "metadata": {},
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf2181cc-a75b-4802-9ce2-cf4f06fef2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-core\n",
      "  Downloading langchain_core-0.2.41-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.8/site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.8/site-packages (from langchain-core) (1.33)\n",
      "Collecting langsmith<0.2.0,>=0.1.112 (from langchain-core)\n",
      "  Downloading langsmith-0.1.136-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.8/site-packages (from langchain-core) (24.0)\n",
      "Collecting pydantic<3,>=1 (from langchain-core)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.8/site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.112->langchain-core)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.112->langchain-core)\n",
      "  Downloading orjson-3.10.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Collecting requests<3,>=2 (from langsmith<0.2.0,>=0.1.112->langchain-core)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.112->langchain-core)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1->langchain-core)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3,>=1->langchain-core)\n",
      "  Downloading pydantic_core-2.23.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core) (4.4.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core) (2024.7.4)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core) (1.3.1)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.112->langchain-core) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.112->langchain-core) (2.2.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.8/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core) (1.2.2)\n",
      "Downloading langchain_core-0.2.41-py3-none-any.whl (397 kB)\n",
      "Downloading langsmith-0.1.136-py3-none-any.whl (296 kB)\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "Downloading orjson-3.10.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: tenacity, requests, pydantic-core, orjson, h11, annotated-types, requests-toolbelt, pydantic, httpcore, httpx, langsmith, langchain-core\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "Successfully installed annotated-types-0.7.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 langchain-core-0.2.41 langsmith-0.1.136 orjson-3.10.7 pydantic-2.9.2 pydantic-core-2.23.4 requests-2.32.3 requests-toolbelt-1.0.0 tenacity-8.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e465f58-1cd1-4051-bd0d-3bfa981d5668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from docx import Document\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = ''\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Extract text from PPTX (using python-pptx)\n",
    "from pptx import Presentation\n",
    "\n",
    "def extract_text_from_pptx(pptx_path):\n",
    "    prs = Presentation(pptx_path)\n",
    "    text = ''\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                text += shape.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Extract relevant sections using keywords\n",
    "def extract_sections(text, keywords):\n",
    "    sections = {}\n",
    "    for keyword in keywords:\n",
    "        sections[keyword] = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            if keyword.lower() in line.lower():\n",
    "                sections[keyword].append(line)\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e83cd1e2-8a7c-4ea7-9df4-5c9068487771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFP Sections: {'Project Overview': ['1 ADA FCCS Project Overview ............................................................................................................................................... 5', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 41 ADA FCCS PROJECT OVERVIEW'], 'Requirements': ['Requirements and Design Document', '2. Requirements & Design Elements .................................................................................................................................... 7', '3. Annexure Requirements & Design Phase .................................................................................................................... 21', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 2ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 3Document Revision History & Approval', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 41 ADA FCCS PROJECT OVERVIEW', 'consolidation for statutory reporting and statutory and management reporting requirements of ADQ', 'ADQ Aviation requirements', '• Ensure future feasibility accommodate management reporting requirements of ADA', 'Aviation reporting requirements.', 'Actual and Management consolidation, as per ADQ Aviation reporting requirements', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 51.4 FCCS HIGH LEVEL COMPARISON – AAS-GRP VS ADA GRP', 'Configurations Configurations as per ADA requirements – Refer 3.1', 'Dimensions Dimensions build as per ADA requirements – Refer Section 2.2 & 3.2', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 62. REQUIREMENTS & DESIGN ELEMENTS', 'Requirements & Design for this project have been grouped in below sections: -', 'requirements of consolidation and reporting', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 7Multi-currency application:', 'This is to keep the application in line with ADQ Aviation requirements', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 8Sr. ADQ Aviation ADA Dimension Details', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 9o These journals for consolidation adjustments will be auto reversing and will reversed', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 10vi. Failures in data load process, will be informed to system admin and respective entity', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 11• Applicable Notes will need to be entered in ADA data forms which have been created using', 'Notes have been configured in ADA application to meet below requirements: -', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 124) ADA Intercompany data will be submitted to FCCS through data input forms', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 13Granular data entered in SDM will be validated with summary account balance data in FCCS.', 'requirements, as statutory data has to be transferred from ADA to AAS application with AAS Inter-', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 14Statutory Reporting Set Up', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 15UniVon will follow approach of deploying default out of box FCCS rules (with minimum changes and', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 16• Data for cash flow will be mapped from below sources', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 17vi. Ratios and KPI’s (upto 10 as scoped in proposal)', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 18adjustments to provide an adequate audit trail for these types of adjustments that can then be viewed', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 192.11 FINANCIAL CLOSE MANAGER, SECURITY AND APPROVAL UNITS', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 203. ANNEXURE REQUIREMENTS & DESIGN PHASE', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 21'], 'Data Integration': ['2.3 Data Integrations and Automations ............................................................................................................................ 9', '2.3.1 Data Integration Approach ............................................................................................................................................ 9', '2.3.2 Data Integration Automations ................................................................................................................................... 10', '3. Data Integration & Automations', '2.3 DATA INTEGRATIONS AND AUTOMATIONS', '2.3.1 DATA INTEGRATION APPROACH', '2.3.2 DATA INTEGRATION AUTOMATIONS'], 'Ownership': ['2.7 Ownership Management & Consolidations ........................................................................................................... 14', '2) Implement Abu Dhabi Aviation Holding entity and ownership structure.', '7. Ownership Management and Consolidation Calculations', 'Close Ownership & Risk Task Close', '• In Built • Ownership', '2.7 OWNERSHIP MANAGEMENT & CONSOLIDATIONS', 'Ownership management has been set up separately for Statutory and Management (ADQ Aviation)', '• Ownership management set up for legal entities in ACTUAL scenario based on details provided', '• Ownership management will be maintained separately for Management and ACTUAL', '• For all other scenarios ownership management will be based on Management scenario', '• Only default out of box Consolidation methods would be configured in Ownership'], 'Reporting': ['Reporting Solution', '2.8 Reporting ........................................................................................................................................................................ 18', '2.8.1 Reporting Using SmartView ....................................................................................................................................... 18', 'consolidation for statutory reporting and statutory and management reporting requirements of ADQ', '• Enable consolidation for statutory reporting purposes', 'and statutory reporting', '• Enable consolidation for statutory and management reporting purposes based on', '• Ensure future feasibility accommodate management reporting requirements of ADA', 'reporting', '4) Enable SmartView based reporting packs', 'Aviation reporting requirements.', '• Facilitate monthly reporting for ADA group.', 'Reporting)', 'Actual and Management consolidation, as per ADQ Aviation reporting requirements', 'Reports Financial Review Pack will be made available for Statutory reporting (Similar', '8. Reporting – SmartView Ad Hoc Reporting pack', 'Integration Layer Application Layer Reporting Layer', 'reporting', 'requirements of consolidation and reporting', 'ADA FCCS application has been enabled for multi-currency reporting. Local/Functional currency for', 'defined. Features of Foreign currency reporting and use of closing rates, average rates and over-ride', '2 Entity Entity ADA statutory structure reporting', '• Calculation of appropriate YTD balances for statutory reporting (depending on legal start date)', 'o Post uploading data for management reporting, audit and other adjustments are', 'expected before finalizing quarterly results for statutory reporting', 'both statutory and management reporting for AAS from the first reporting period after', '• Mapping meets both Statutory and Management reporting needs', '• Alternate accounts structure as needed for Management reporting (ADQ Aviation) will be', '6) Revenue classification required for IFRS reporting will be submitted to FCCS through data', 'reporting scenario', 'ADA FCCS – REQUIREMENTS AND DESIGN DOCUMENT 14Statutory Reporting Set Up', 'Management Reporting Set Up', '(Statutory Reporting) scenario', '• Consolidation of Actual (Statutory Reporting), Budget, Forecast and management reporting', '• ADA team will share Financial and Operational KPI’s specific to ADA reporting.', '2.8 REPORTING', '2.8.1 REPORTING USING SMARTVIEW', 'Reporting needed by ADA for Statutory purposes will be enabled through SmartView reporting pack.', 'Management reporting will be developed by ADA team using SmartView ad-hoc analysis.', 'perform reporting reclassifications, make adjustments, or alter data after a trial balance load has been']}\n",
      "Response Sections: {'Business Value': ['Business Value Through this Implementation', 'Business Value Themes', 'Business Value Through this Implementation', 'Business Value', 'Business Value'], 'Data Mart': ['Oracle Data Mart & Analytics with LLM powered Oracle Chat Bot', \"Oracle Consulting will implement  Oracle chatbot, powered by state-of-the-art Large Language Model (LLM) technology, offers advanced conversational capabilities, enabling businesses to interact with data in a more intuitive and efficient manner. Integrated seamlessly with Oracle's robust Data Mart and analytics platform, this solution transforms the way Abu Dhabi Aviation can access, analyze, and leverage their data.\", 'Key Features include Advanced Conversational AI Leveraging LLM technology, Seamless Integration with Oracle Data Mart coupled with Powerful Oracle Analytics Dashboards and Reports.', 'The chatbot is fully integrated with Oracle’s Data Mart, enabling real-time data access and interactions. ', 'Data Mart with AI Powered Oracle Chat', '1. Data Mart [FCCS]', '2.  Data Mart ADA, EYE, AMMROC, GAL [Procurement, Inventory & HR ]', 'Scope\\x0bData Mart and LLM Powered Oracle Chat  ', 'Data Mart and LLM Powered Oracle Chat', 'Development of Data Mart for FCCS', 'Development of Data Mart for Great Plains & Rusada ', 'Size of Data Mart is up to 1 TB or 4 years whichever is lesser for production environment. Non-production environment the size is 100 GB or 1 year  ', 'Source Data Extraction to Data Mart is based on SQL Queries executed by ADA on the source system and for files to be provided at target location on OCI Object Store'], 'LLM': ['Oracle Data Mart & Analytics with LLM powered Oracle Chat Bot', \"Oracle Consulting will implement  Oracle chatbot, powered by state-of-the-art Large Language Model (LLM) technology, offers advanced conversational capabilities, enabling businesses to interact with data in a more intuitive and efficient manner. Integrated seamlessly with Oracle's robust Data Mart and analytics platform, this solution transforms the way Abu Dhabi Aviation can access, analyze, and leverage their data.\", 'Key Features include Advanced Conversational AI Leveraging LLM technology, Seamless Integration with Oracle Data Mart coupled with Powerful Oracle Analytics Dashboards and Reports.', 'Leveraging LLM technology, the Oracle chatbot understands and generates human-like text, allowing for sophisticated and nuanced interactions. ', 'OCI GPU for LLM', '3. LLM Training  for Oracle Chat', '2 LLMs testing ', 'Fine Tuning & Prompt Engineering of LLMs', 'Enabling ODA and integrating with LLM', 'Integrating with LLM ', 'Scope\\x0bData Mart and LLM Powered Oracle Chat  ', 'Data Mart and LLM Powered Oracle Chat', 'Implement  Oracle chatbot, powered by state-of-the-art Large Language Model (LLM) technology, offers advanced conversational capabilities, enabling businesses to interact with data in a more intuitive and efficient manner', 'LLM Model Training', 'LLM', 'LLM selection subject to change during the implementation. LLM Model Accuracy is based on training sets; however, the accuracy parameters will be accepted once they published & reviewed jointly by You & Oracle  ', 'Licenses of 3rd party and its related services including LLM and Avatar platforms  '], 'Reporting': [], 'Deliverables': ['Scope & Deliverables', 'Deliverables '], 'Scope': ['Scope & Deliverables', 'Scope\\x0bEnvironment setup', 'Technical Scope', 'Scope\\x0bData Mart and LLM Powered Oracle Chat  ', 'Technical Scope', 'Change Control Board will be responsible for evaluating any change in scope after consulting with the respective business owners.', 'Out of Scope'], 'Assumptions': ['Assumptions & Obligations', 'Assumptions ', 'Assumptions ']}\n"
     ]
    }
   ],
   "source": [
    "# Load RFP and RFP Response\n",
    "rfp_text = extract_text_from_pdf('rfp.pdf')\n",
    "rfp_response_text = extract_text_from_pptx('rfp_response.pptx')\n",
    "\n",
    "# Define sections to extract\n",
    "keywords_rfp = ['Project Overview', 'Requirements', 'Data Integration', 'Ownership', 'Reporting']\n",
    "keywords_response = ['Business Value', 'Data Mart', 'LLM', 'Reporting', 'Deliverables', 'Scope', 'Assumptions']\n",
    "\n",
    "# Extract sections\n",
    "rfp_sections = extract_sections(rfp_text, keywords_rfp)\n",
    "response_sections = extract_sections(rfp_response_text, keywords_response)\n",
    "\n",
    "# Check extracted sections\n",
    "print(\"RFP Sections:\", rfp_sections)\n",
    "print(\"Response Sections:\", response_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1748b1-5f8f-4e46-b6a6-fe8e98eb97a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Project Overview and Business Value: 0.00\n",
      "Similarity between Project Overview and Data Mart: 0.07\n",
      "Similarity between Project Overview and LLM: 0.03\n",
      "Similarity between Project Overview and Reporting: 0.00\n",
      "Similarity between Project Overview and Deliverables: 0.00\n",
      "Similarity between Requirements and Business Value: 0.01\n",
      "Similarity between Requirements and Data Mart: 0.18\n",
      "Similarity between Requirements and LLM: 0.14\n",
      "Similarity between Requirements and Reporting: 0.00\n",
      "Similarity between Requirements and Deliverables: 0.00\n",
      "Similarity between Data Integration and Business Value: 0.00\n",
      "Similarity between Data Integration and Data Mart: 0.28\n",
      "Similarity between Data Integration and LLM: 0.16\n",
      "Similarity between Data Integration and Reporting: 0.00\n",
      "Similarity between Data Integration and Deliverables: 0.00\n",
      "Similarity between Ownership and Business Value: 0.00\n",
      "Similarity between Ownership and Data Mart: 0.07\n",
      "Similarity between Ownership and LLM: 0.08\n",
      "Similarity between Ownership and Reporting: 0.00\n",
      "Similarity between Ownership and Deliverables: 0.00\n",
      "Similarity between Reporting and Business Value: 0.00\n",
      "Similarity between Reporting and Data Mart: 0.10\n",
      "Similarity between Reporting and LLM: 0.09\n",
      "Similarity between Reporting and Reporting: 0.00\n",
      "Similarity between Reporting and Deliverables: 0.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer().fit_transform([text1, text2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors)[0, 1]\n",
    "\n",
    "# Map RFP and Response sections\n",
    "for rfp_key, rfp_value in rfp_sections.items():\n",
    "    for response_key, response_value in response_sections.items():\n",
    "        similarity = calculate_similarity(' '.join(rfp_value), ' '.join(response_value))\n",
    "        print(f'Similarity between {rfp_key} and {response_key}: {similarity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d76d978c-46b0-4e0a-affc-9da053623297",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Chat Result**************************\n",
      "Here is a suggested process flow diagram for the implementation of the Oracle Data Mart and Analytics platform with an LLM-powered Oracle Chatbot:\n",
      "\n",
      "## High-Level Process Flow:\n",
      "\n",
      "### 1. Project Initiation:\n",
      "- Kick-off meeting with ADA and Oracle teams.\n",
      "- Define project scope, objectives, and timelines.\n",
      "- Assign roles and responsibilities.\n",
      "\n",
      "### 2. Data Mart Development:\n",
      "- Source data extraction from FCCS, Procurement, and HR systems.\n",
      "- Data cleansing and validation.\n",
      "- Data modeling and transformation.\n",
      "- Create data marts for FCCS, ADA, EYE, AMMROC, and GAL.\n",
      "\n",
      "### 3. LLM Model Training:\n",
      "- Select and fine-tune LLM models.\n",
      "- Train LLM models on ADA's data and use cases.\n",
      "- Test and validate LLM performance.\n",
      "\n",
      "### 4. Oracle Digital Assistant (ODA) Setup:\n",
      "- Integrate ODA with LLM models.\n",
      "- Configure ODA chat interface and avatar system.\n",
      "- Train ADA team on ODA usage.\n",
      "\n",
      "### 5. Testing and User Acceptance:\n",
      "- Conduct unit testing, integration testing, and UAT.\n",
      "- Address feedback and refine the solution.\n",
      "\n",
      "### 6. Go-Live and Support:\n",
      "- Finalize production setup and cutover.\n",
      "- Provide go-live and post-production support.\n",
      "- Conduct knowledge transfer sessions.\n",
      "\n",
      "## Detailed Process Flow:\n",
      "\n",
      "### 1. Project Initiation:\n",
      "- Oracle and ADA teams meet to kick off the project.\n",
      "- Discuss and finalize project scope, objectives, timelines, and deliverables.\n",
      "- Assign roles and responsibilities to team members.\n",
      "- Develop a project plan and schedule.\n",
      "\n",
      "### 2. Data Mart Development:\n",
      "- Extract data from source systems: FCCS, Procurement, and HR (Great Plains, Rusada, Adrenalin, Jaggaer, AIMS).\n",
      "- Perform data cleansing and validation to ensure data quality.\n",
      "- Design and create data marts for FCCS, ADA, EYE, AMMROC, and GAL.\n",
      "- Define data models and transform data to fit the data marts.\n",
      "\n",
      "### 3. LLM Model Training:\n",
      "- Select and fine-tune LLM models based on ADA's requirements.\n",
      "- Train LLM models on ADA's data and specific use cases.\n",
      "- Test and validate LLM performance and accuracy.\n",
      "- Optimize and fine-tune LLM models based on test results.\n",
      "\n",
      "### 4. Oracle Digital Assistant (ODA) Setup:\n",
      "- Integrate ODA with trained LLM models.\n",
      "- Configure ODA chat interface and avatar system according to ADA's branding and requirements.\n",
      "- Train-the-trainer sessions for ADA team on ODA usage and administration.\n",
      "\n",
      "### 5. Testing and User Acceptance\n"
     ]
    }
   ],
   "source": [
    "import oci\n",
    "import pdfplumber\n",
    "from pptx import Presentation\n",
    "\n",
    "# Setup basic variables\n",
    "compartment_id = \"ocid1.compartment.oc1..aaaaaaaaretksgipt3jgwfpzgh4ijyw54uynyfviaxs5li4wtl744fj4fi3q\"\n",
    "CONFIG_PROFILE = \"DEFAULT\"\n",
    "config = oci.config.from_file('config', CONFIG_PROFILE)\n",
    "\n",
    "# Service endpoint\n",
    "endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config, service_endpoint=endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "\n",
    "# Function to extract text from PDF (RFP)\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = ''\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from PPTX (RFP response)\n",
    "def extract_text_from_pptx(pptx_path):\n",
    "    prs = Presentation(pptx_path)\n",
    "    text = ''\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                text += shape.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Load and extract text from RFP and RFP Response\n",
    "rfp_text = extract_text_from_pdf('rfp.pdf')\n",
    "rfp_response_text = extract_text_from_pptx('rfp_response.pptx')\n",
    "\n",
    "# Concatenate the two documents for context\n",
    "combined_input = f\"RFP Document:\\n{rfp_text}\\n\\nRFP Response Document:\\n{rfp_response_text}\"\n",
    "\n",
    "# Function to get response from Generative AI API\n",
    "def ask_question(question, input_text):\n",
    "    chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "\n",
    "    # Create chat request\n",
    "    chat_request = oci.generative_ai_inference.models.CohereChatRequest()\n",
    "    chat_request.message = f\"{question}\\n\\nContext:\\n{input_text}\"\n",
    "    chat_request.max_tokens = 600\n",
    "    chat_request.temperature = 0.25\n",
    "    chat_request.frequency_penalty = 0\n",
    "    chat_request.top_p = 0.75\n",
    "    chat_request.top_k = 0\n",
    "\n",
    "    # Set chat details and compartment info\n",
    "    chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=\"ocid1.generativeaimodel.oc1.us-chicago-1.amaaaaaask7dceya7ozidbukxwtun4ocm4ngco2jukoaht5mygpgr6gq2lgq\")\n",
    "    chat_detail.chat_request = chat_request\n",
    "    chat_detail.compartment_id = compartment_id\n",
    "\n",
    "    # Send request and get response\n",
    "    chat_response = generative_ai_inference_client.chat(chat_detail)\n",
    "\n",
    "    # Extract the message from the chat history\n",
    "    chat_history = chat_response.data.chat_response.chat_history\n",
    "    if chat_history:\n",
    "        return chat_history[-1].message  # Return the last message from the chatbot\n",
    "\n",
    "    return \"No response received.\"\n",
    "\n",
    "# Example question to ask based on RFP and RFP Response\n",
    "question = \"Come up with a process flow diagram suitable for the application that has been asked by the client?\"\n",
    "response = ask_question(question, combined_input)\n",
    "\n",
    "print(\"**************************Chat Result**************************\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec5938dc-35d8-4f1a-ac82-44356ce0278b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Chat Result**************************\n",
      "Question: What is the purpose and scope of this project as outlined in the RFP?\n",
      "Response: The purpose of this project, as outlined in the RFP, is to implement a consolidation and reporting solution for Abu Dhabi Aviation Holding, leveraging Oracle EPM Cloud FCCS. The scope of the project includes:\n",
      "\n",
      "- Enabling consolidation for statutory and management reporting, including actual, budget, forecast, and commentary scenarios.\n",
      "- Implementing the Abu Dhabi Aviation Holding entity and ownership structure.\n",
      "- Automating data integrations and loads for all entities.\n",
      "- Enabling SmartView-based reporting packs.\n",
      "- Configuring standard out-of-box ratios and KPIs, as well as additional ratios for operational reporting.\n",
      "- Setting up ownership management and consolidations for statutory and management reporting.\n",
      "- Developing supplemental data forms and managing intercompany data.\n",
      "- Providing training and support for the use of dashboards and other features.\n",
      "\n",
      "The project also includes the implementation of an Oracle chatbot, powered by Large Language Model (LLM) technology, integrated with Oracle's Data Mart and analytics platform. This enables advanced conversational capabilities and enhances the user experience for data interactions and analysis.\n",
      "\n",
      "**************************Chat Result**************************\n",
      "Question: What are the main business objectives that this project aims to achieve for Abu Dhabi Aviation Holding?\n",
      "Response: The main business objectives of the project for Abu Dhabi Aviation Holding are to:\n",
      "\n",
      "- Facilitate monthly reporting for the ADA group.\n",
      "- Facilitate monthly data movements from ADA to AAS (Statutory and Management Reporting).\n",
      "- Enable consolidation of Budget and Forecast data, in addition to Actual and Management consolidation, as per ADQ Aviation reporting requirements.\n",
      "- Facilitate variance analysis.\n",
      "- Implement Oracle EPM Cloud FCCS to meet consolidation for statutory reporting and statutory and management reporting requirements of ADQ Aviation.\n",
      "- Enable consolidation for statutory reporting purposes.\n",
      "- Enable Consolidation based on group structure.\n",
      "- Make history available for 2023 and up to current periods.\n",
      "- Enable supplemental data manager to enable needed granularity of management and statutory reporting.\n",
      "- Ensure future feasibility to accommodate management reporting requirements of the ADA group.\n",
      "- Configure standard out-of-box ratios and KPIs, with additional ratios for operational reporting.\n",
      "\n",
      "**************************Chat Result**************************\n",
      "Question: What are the key functional requirements that the solution must fulfill, including features and modules needed for reporting and consolidation?\n",
      "Response: Here is a list of key functional requirements that the solution must fulfill, including features and modules needed for reporting and consolidation:\n",
      "\n",
      "- **Multi-currency support:** The application should enable multi-currency reporting, with the ability to set the local/functional currency for each entity. This ensures accurate currency translations during roll-up and reporting.\n",
      "- **Data integration and automation:** Implement a seamless data integration process, including standardized file formats, automated data loads, and a secure file-sharing process. Enable quarterly adjustments and journal entries for statutory data.\n",
      "- **Supplemental Data Manager (SDM):** Utilize SDM to capture additional breakups and disclosures of financial information. Ensure data validation between SDM and account balances in the application.\n",
      "- **Intercompany transactions:** Activate all base entities for inter-company transactions. Retain the existing inter-company structure for statutory data transfer.\n",
      "- **Ownership management and consolidations:** Set up ownership management separately for statutory and management reporting scenarios. Deploy default consolidation rules and methods, with customizations as needed for specific ADA scenarios.\n",
      "- **Consolidation calculations:** Automate consolidation calculations, including equity accounting, goodwill offset, cash flow management, SOCIE, and purchase price adjustments.\n",
      "- **Reporting:** Enable reporting through SmartView reporting packs for statutory purposes. Provide ad-hoc analysis capabilities for management reporting. Configure dashboards for close management, compliance, and financial insights.\n",
      "- **Journal module:** Provide journal functionality for topside adjustments, with features such as standard templates, data load and extraction, period management, and workflow capabilities.\n",
      "- **Valid intersections:** Implement valid intersections to ensure data integrity and optimize the close process. Prevent users from entering data in invalid cell combinations.\n",
      "- **Financial close management, security, and approval units:** Utilize close manager features to monitor monthly, quarterly, and yearly close cycles. Implement a structured security matrix, providing access to entity and corporate users.\n",
      "- **Dashboards and analytics:** Configure out-of-the-box dashboards for close management, compliance, and financial insights. Provide training to ADA Aviation team to build additional dashboards as needed.\n",
      "- **Data sources and extensions:** Currently, the data sources include Oracle FCCS, Procurement, and HR for four entities: ADA, EYE, AMMROC, and GAL.\n",
      "- **LLM-powered Oracle Chat:** Implement a chatbot integrated with the Data Mart, leveraging Large Language Model (LLM) technology for advanced conversational capabilities.\n",
      "- **Oracle Analytics Cloud:** Utilize Oracle Analytics Cloud for insightful dashboards and reports, with a data model tailored to ADA's business needs.\n",
      "- **Environment setup:** Establish two environments, Non-Prod and Prod, on OCI, including necessary services such as Oracle Data Integration, Autonomous Database, Oracle Digital Assistant, Object Storage, and more.\n",
      "- **Training and support:** Provide train-the-trainer sessions, UAT support, and go-live support to ensure a smooth transition\n",
      "\n",
      "**************************Chat Result**************************\n",
      "Question: What are the key non-functional requirements, such as performance, scalability, availability, and maintainability of the system?\n",
      "Response: Performance, scalability, availability, and maintainability are crucial non-functional requirements for any system, and they are often interrelated. Here are the key considerations for each:\n",
      "\n",
      "Performance:\n",
      "- Response Time: The system should provide timely responses to user queries and interactions. Response time should be within acceptable limits, ensuring a smooth and efficient user experience.\n",
      "- Throughput: The system should be able to handle a specified number of transactions or operations within a given time frame. This includes handling peak loads effectively without significant degradation in performance.\n",
      "- Efficiency: The system should utilize resources efficiently, including computational resources, memory, and network bandwidth. Optimized resource usage ensures the system can scale and perform effectively.\n",
      "\n",
      "Scalability:\n",
      "- Capacity: The system should be designed to accommodate growth in terms of user base, data volume, and transaction rates. It should be able to scale horizontally or vertically to handle increased demand.\n",
      "- Elasticity: The system should be able to adapt to changing workloads dynamically. This includes the ability to provision or de-provision resources automatically based on demand, ensuring optimal utilization.\n",
      "- Extensibility: The system should be designed with a modular architecture that allows for easy expansion or enhancement of features and functionalities over time.\n",
      "\n",
      "Availability:\n",
      "- Uptime: The system should have high availability, minimizing unplanned downtime. This includes implementing redundancy, fault tolerance, and disaster recovery mechanisms to ensure continuous operation.\n",
      "- Reliability: The system should be reliable, delivering consistent and correct results. This includes handling errors gracefully, implementing robust error recovery mechanisms, and ensuring data integrity.\n",
      "- Resiliency: The system should be resilient to failures, able to recover quickly and maintain service continuity. This includes implementing backup and recovery strategies, data replication, and system monitoring.\n",
      "\n",
      "Maintainability:\n",
      "- Modifiability: The system should be designed for easy modification and adaptation to changing requirements. This includes using flexible and extensible design patterns, modular components, and well-documented code.\n",
      "- Serviceability: The system should facilitate easy maintenance and support. This includes providing comprehensive monitoring, logging, and troubleshooting capabilities, as well as clear documentation for administrators and support staff.\n",
      "- Testability: The system should be designed with testability in mind, incorporating test automation, continuous integration, and deployment practices. This ensures that changes and updates can be validated efficiently and effectively.\n",
      "\n",
      "These non-functional requirements are essential for building a robust, scalable, and reliable system. They ensure that the system can handle current and future demands, provide a positive user experience, and facilitate efficient maintenance and support.\n",
      "\n",
      "**************************Chat Result**************************\n",
      "Question: Can you describe the overall solution architecture, including the key components, integrations, and data flows involved in this project?\n",
      "Response: Here is an overview of the solution architecture, including key components, integrations, and data flows for the project:\n",
      "\n",
      "**Overall Solution Architecture:**\n",
      "- The project involves implementing a Large Language Model (LLM)-powered Oracle chatbot integrated with a Data Mart and analytics platform.\n",
      "- The Oracle chatbot understands and generates human-like text, providing advanced conversational capabilities.\n",
      "- The chatbot is seamlessly integrated with the Data Mart, enabling real-time data access and interactions.\n",
      "- Oracle Analytics Cloud provides insightful dashboards and reports for data visualization and analysis.\n",
      "\n",
      "**Key Components:**\n",
      "- Oracle Cloud Infrastructure (OCI): Provides the underlying infrastructure for the solution, including compute, storage, networking, and database services.\n",
      "- Oracle Data Mart: Serves as the central repository for data from various sources, including FCCS, Procurement, and HR systems.\n",
      "- LLM-Powered Oracle Chat: Leverages advanced conversational AI technology, allowing users to interact with data through natural language queries.\n",
      "- Oracle Analytics Cloud (OAC): Offers powerful analytics capabilities, enabling the creation of dashboards, reports, and data models.\n",
      "- Other Components: Avatar system for text-to-speech and speech-to-text, Oracle Digital Assistant (ODA), Oracle API Gateway, and Oracle Identity Access Management Cloud Services for security.\n",
      "\n",
      "**Integrations:**\n",
      "- Data Sources: The solution integrates with various data sources, including FCCS, Procurement, HR, and other systems (Great Plains, Rusada, Adrenalin, Jaggaer Procurement, AIMS).\n",
      "- Data Mart: The Data Mart aggregates and stores data from these sources, providing a centralized repository for analytics.\n",
      "- LLM-Powered Oracle Chat: The chatbot integrates with the Data Mart to access and interact with the data, enabling users to query, receive responses, and perform data-related tasks through conversational inputs.\n",
      "- OAC: OAC integrates with the Data Mart to access the data and provide analytics capabilities, such as dashboard creation and reporting.\n",
      "\n",
      "**Data Flows:**\n",
      "- Data Extraction: Data is extracted from the source systems and loaded into the Data Mart using SQL queries and file-based extracts.\n",
      "- Data Mart: The Data Mart serves as the central hub for data storage, transformation, and modeling. It provides a structured and organized view of the data for analysis.\n",
      "- LLM-Powered Oracle Chat: The chatbot interacts with the Data Mart to retrieve and manipulate data based on user queries. It generates responses and performs data-related tasks as instructed by the user.\n",
      "- OAC: OAC accesses the data from the Data Mart and enables users to create dashboards, generate reports, and perform advanced analytics.\n",
      "- Avatar System: The avatar system integrates with the LLM-powered Oracle Chat to provide text-to-speech and speech-to-text capabilities, enhancing the user experience.\n",
      "\n",
      "This solution architecture outlines the key components, integrations, and data flows involved in the project\n",
      "\n",
      "**************************Chat Result**************************\n",
      "Question: What are the specific integration requirements between Oracle FCCS and external systems like Great Plains, Rusada, and others?\n",
      "Response: I can see that you have provided a large amount of text, but I am unable to find any specific integration requirements between Oracle FCCS and external systems like Great Plains or Rusada. Could you please point me to the relevant section or provide additional context so that I can better assist you?\n",
      "\n",
      "**************************Chat Result**************************\n",
      "Question: What is the data management strategy, including data storage, retention, and backup policies for this project?\n",
      "Response: The data management strategy for this project involves the implementation of a Data Mart, which serves as a centralized repository for data from various sources, including Oracle FCCS, Procurement, and HR systems for four entities: ADA, EYE, AMMROC, and GAL. The Data Mart will be designed with a size of up to 1 TB or 4 years of data for the production environment and 100 GB or 1 year of data for the non-production environment.\n",
      "\n",
      "Data extraction for the Data Mart will be based on SQL queries executed by ADA on the source systems, and files will be provided at a target location on OCI Object Storage. ADA is responsible for data cleansing and ensuring data quality, and they will provide clean data at the start of the project.\n",
      "\n",
      "For data storage, Oracle Cloud Infrastructure (OCI) services will be utilized, providing a secure and scalable environment for the Data Mart and related components. OCI offers robust data security, including encryption at rest and in transit, as well as access control mechanisms to protect sensitive information.\n",
      "\n",
      "Regarding data retention, the specific policies are not outlined in the provided documents. However, it is important to note that data retention requirements may vary depending on regulatory and compliance standards applicable to the organization and the specific data being managed. ADA should establish data retention policies that align with their industry and legal obligations.\n",
      "\n",
      "For data backup, the documents mention that further maintenance, including backup procedures, will be the responsibility of ADA's team after post-production support. This suggests that Oracle will provide initial guidance or setup for data backup, but ADA will ultimately be accountable for ongoing backup processes and ensuring data recoverability.\n",
      "\n",
      "Overall, the data management strategy for this project focuses on consolidating data from multiple sources into a Data Mart, leveraging OCI for storage, and relying on ADA to ensure data quality and perform data cleansing. The specific data retention and backup policies are not explicitly defined but are important considerations for ADA to address in accordance with their regulatory and compliance requirements.\n",
      "\n",
      "**************************Chat Result**************************\n",
      "Question: What are the data security protocols, including user access control, encryption methods, and audit logging mechanisms?\n",
      "Response: Data security protocols are essential to protect sensitive information and ensure its integrity and confidentiality. Here are the key protocols, including user access control, encryption methods, and audit logging mechanisms:\n",
      "\n",
      "## User Access Control:\n",
      "- Role-Based Access Control (RBAC): Access to data is restricted based on user roles and responsibilities. Different roles are assigned to specific permissions, ensuring that users only have access to the data necessary for their tasks.\n",
      "- Multi-Factor Authentication (MFA): Users are required to provide multiple forms of identification before gaining access to sensitive data. This could include passwords, biometric data, or physical tokens.\n",
      "- Privileged Access Management (PAM): Special controls and monitoring are implemented for users with elevated privileges, such as system administrators, to prevent unauthorized access or misuse of privileges.\n",
      "\n",
      "## Encryption Methods:\n",
      "- Data-at-Rest Encryption: Data stored on servers or databases is encrypted using secure encryption algorithms (e.g., AES 256-bit). This ensures that even if unauthorized access is gained, the data remains unreadable.\n",
      "- Data-in-Transit Encryption: Data transmitted over networks, such as the internet, is encrypted using secure protocols like TLS (Transport Layer Security) or VPN (Virtual Private Network) connections.\n",
      "- Key Management: Encryption keys are securely generated, stored, and managed to ensure the confidentiality and integrity of encrypted data.\n",
      "\n",
      "## Audit Logging Mechanisms:\n",
      "- Activity Monitoring: All user activities, including data access, modifications, and deletions, are logged and monitored. This helps detect and investigate suspicious activities, ensuring accountability and deterring unauthorized access.\n",
      "- Alerting and Reporting: The system generates alerts and reports on suspicious activities, such as failed login attempts or unauthorized access attempts, allowing for prompt investigation and response.\n",
      "- Audit Trail Retention: Audit logs are stored securely and retained for a specified period to comply with regulatory requirements and facilitate forensic investigations if needed.\n",
      "\n",
      "These data security protocols form a comprehensive framework to protect sensitive data throughout its lifecycle, ensuring confidentiality, integrity, and availability. They are designed to mitigate risks, detect and respond to threats, and facilitate compliance with relevant data privacy and security regulations.\n",
      "\n",
      "**************************Chat Result**************************\n",
      "Question: What are the automation and workflow requirements for processes like data consolidation, variance analysis, and reporting?\n",
      "Response: Automation and workflow requirements for data consolidation, variance analysis, and reporting processes are critical to ensuring efficient and effective operations. Here are the key requirements for each of these processes:\n",
      "\n",
      "## Data Consolidation:\n",
      "- Automation:\n",
      "   - Data Integration: Automate the process of integrating data from multiple sources. This includes developing data integration processes, establishing data mapping and validation rules, and creating automated data load processes.\n",
      "   - File Sharing: Implement a secure and efficient file-sharing process, including standard file naming conventions and a centralized file storage system.\n",
      "   - Data Transformation: Ensure data is transformed and standardized to a consistent format suitable for analysis and reporting.\n",
      "\n",
      "- Workflow:\n",
      "   - Data Ownership: Clearly define data ownership and responsibilities for data preparation, validation, and loading.\n",
      "   - Data Quality Checks: Implement data quality checks and validation processes to identify and rectify data inconsistencies, errors, or missing values.\n",
      "   - Data Security: Establish data security protocols to protect sensitive data during transmission and storage.\n",
      "\n",
      "## Variance Analysis:\n",
      "- Automation:\n",
      "   - Data Comparison: Automate the process of comparing actual data with budget, forecast, or previous period data to identify variances.\n",
      "   - Alerting and Notification: Set up automated alerts or notifications when variances exceed predefined thresholds, triggering further investigation or action.\n",
      "\n",
      "- Workflow:\n",
      "   - Variance Analysis Process: Define a step-by-step process for analyzing variances, including root cause analysis, impact assessment, and decision-making.\n",
      "   - Collaboration and Communication: Establish a workflow for collaboration and communication between finance, operations, and other relevant teams during the variance analysis process.\n",
      "\n",
      "## Reporting:\n",
      "- Automation:\n",
      "   - Report Generation: Automate the generation of standard reports, including financial statements, management reports, and regulatory reports.\n",
      "   - Report Distribution: Set up automated distribution of reports to relevant stakeholders, ensuring timely and secure delivery.\n",
      "\n",
      "- Workflow:\n",
      "   - Report Approval: Implement a report approval process, including review, validation, and sign-off by authorized individuals.\n",
      "   - Ad-hoc Reporting: Establish a process for generating ad-hoc reports or custom analyses, including data access and query capabilities for authorized users.\n",
      "\n",
      "Overall, the automation and workflow requirements for these processes aim to streamline data handling, improve accuracy, enhance collaboration, and ensure timely and efficient reporting. These requirements should be tailored to the specific needs and systems of the organization, ensuring a seamless and effective data management and reporting process.\n",
      "\n",
      "**************************Chat Result**************************\n",
      "Question: What are the specific reporting requirements, including the use of SmartView, dashboards, and other tools for ad-hoc reporting?\n",
      "Response: Here is a summary of the specific reporting requirements, including the use of SmartView, dashboards, and other tools for ad-hoc reporting, based on the provided context:\n",
      "\n",
      "**Reporting Requirements:**\n",
      "- **SmartView:** SmartView is a tool that enables ad-hoc reporting and analysis. It is used for generating the reporting packs for statutory purposes. It allows users to access and analyze data directly from the FCCS application.\n",
      "- **Dashboards:** Currently, dashboards are not in use, but the plan is to configure two out-of-the-box dashboards for close management and compliance. Additionally, one financial dashboard will be built. Users will need to log in to the system to access these dashboards.\n",
      "- **Other Tools:** The solution also includes the Oracle Data Mart and Analytics platform, which provides powerful dashboards and reports. The Large Language Model (LLM)-powered Oracle Chatbot seamlessly integrates with the Data Mart, allowing users to interact with data through simple conversational inputs.\n",
      "\n",
      "The reporting requirements focus on utilizing SmartView for ad-hoc reporting and statutory purposes, while also incorporating dashboards and leveraging the capabilities of the Oracle Data Mart and Analytics platform for enhanced data interaction and visualization.\n",
      "\n",
      "**************************Chat Result**************************\n",
      "Question: What are the assumptions and constraints identified for this project, including dependencies on other systems and limitations of the current environment?\n",
      "Response: **Assumptions:**\n",
      "- The project will be implemented using Oracle's Cloud Infrastructure (OCI) services in both Non-Prod and Prod tenancies.\n",
      "- Data Mart will be developed for FCCS, with up to 10 file extracts, 200 attributes, 5 subject areas, and 10 reports.\n",
      "- Data Mart for ADA, EYE, AMMROC, and GAL will include up to 17 file extracts from various sources, 340 attributes, 5 subject areas, and 15 reports.\n",
      "- LLM training will involve testing 2 LLMs, fine-tuning, and prompt engineering.\n",
      "- Oracle Digital Assistant (ODA) will be enabled and integrated with the LLM.\n",
      "- An avatar system with text-to-speech and speech-to-text models will be integrated with the LLM.\n",
      "- Up to 6 train-the-trainer sessions will be conducted.\n",
      "- UAT and go-live support will be provided for 2 weeks each.\n",
      "- The implementation efforts and timelines are subject to change and will be reviewed during the project's lifecycle.\n",
      "- Data quality is checked and confirmed by ADA.\n",
      "- ADA will handle data cleansing in the source systems and provide clean data at the project's start.\n",
      "- Relevant individuals from ADA will form a core team to provide functional and technical inputs.\n",
      "- LLM selection may change during implementation. LLM accuracy parameters will be accepted after joint review by ADA and Oracle.\n",
      "- Avatar customization will follow ADA branding guidelines, with one Apex page for ODA and the 3rd party avatar solution.\n",
      "- The OCI native services will be prioritized over other technologies.\n",
      "- A dedicated project manager will be available from ADA for this engagement.\n",
      "- ADA will be responsible for organizational change management and user training.\n",
      "- ADA will provide Oracle with full access to relevant resources and ensure adequate skills and knowledge.\n",
      "- ADA will coordinate with vendors for network connectivity.\n",
      "- ADA will perform necessary 3rd party/configuration changes.\n",
      "- ADA will handle further maintenance (backup, monitoring, housekeeping, etc.) after post-production support.\n",
      "- ADA will ensure daily incremental data availability for file-based source systems.\n",
      "- The Change Control Board will evaluate any scope changes in consultation with business owners.\n",
      "\n",
      "**Constraints and Dependencies:**\n",
      "- ADA is responsible for procuring licenses for the avatar system and related 3rd party licenses.\n",
      "- ADA is responsible for providing source data structure and access to 3rd party systems like Adrenalin, Great Plains, Rusada, AIMS, and Jagger.\n",
      "- ADA is responsible for all required communications and approvals for data sharing with any entity/sub-entity.\n",
      "- ADA is responsible for data cleansing, data quality checks, and source system analysis.\n",
      "- ADA will manage OCI resource ramp-up/ramp-down and licenses for 3rd party services,\n",
      "\n",
      "**************************Chat Result**************************\n",
      "Question: What are the key deliverables and milestones for this project, including UAT, go-live support, and post-implementation support?\n",
      "Response: Here is a list of key deliverables and milestones for the project, including UAT, go-live support, and post-implementation support:\n",
      "\n",
      "**Key Deliverables:**\n",
      "- Data Mart with AI-Powered Oracle Chat: Set up a state-of-the-art infrastructure to implement advanced conversational capabilities.\n",
      "- Data Mart for FCCS: Extract and load data from FCCS to create a centralized data repository.\n",
      "- Data Mart for Additional Sources (Procurement, Inventory, and HR): Integrate data from Great Plains, Rusada, Adrenalin, Jaggaer Procurement, and AIMS.\n",
      "- LLM Training and Integration: Train and integrate Large Language Models (LLMs) with the Oracle Digital Assistant (ODA) to enable advanced conversational AI.\n",
      "- ODA Chat Interface and Avatar: Develop a user-friendly chat interface and integrate it with the avatar system for a seamless user experience.\n",
      "- Reports and Dashboards: Create insightful reports and dashboards using Oracle Analytics Cloud (OAC) to visualize data and support decision-making.\n",
      "- UAT Support: Provide support during the User Acceptance Testing (UAT) phase to ensure the solution meets business requirements.\n",
      "- Go-Live Support: Assist with the go-live process to ensure a smooth transition to the new system.\n",
      "- Post-Implementation Support: Offer ongoing support post-implementation to address any issues and ensure the stability of the solution.\n",
      "\n",
      "**Milestones:**\n",
      "- Project Kick-off: Initiate the project with a kick-off meeting to establish project goals, timelines, and responsibilities.\n",
      "- Requirement Gathering: Conduct requirement gathering sessions to understand business needs and scope.\n",
      "- Solution Design: Design the solution architecture and create a detailed plan for implementation.\n",
      "- Development: Develop the data mart, integrate data sources, train LLMs, and build the chat interface and avatar system.\n",
      "- UAT: Conduct user acceptance testing to ensure the solution meets business requirements.\n",
      "- Go-Live: Assist with the go-live process and provide support during the initial live usage of the system.\n",
      "- Post-Implementation Support: Provide ongoing support post-implementation to address any issues and ensure the long-term stability of the solution.\n",
      "\n",
      "These deliverables and milestones outline the key tasks and phases of the project. The timeline and effort estimates provided are indicative and may be subject to change based on the specific requirements and complexities that arise during the implementation process.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import oci\n",
    "import pdfplumber\n",
    "from pptx import Presentation\n",
    "\n",
    "# Setup basic variables\n",
    "compartment_id = \"ocid1.compartment.oc1..aaaaaaaaretksgipt3jgwfpzgh4ijyw54uynyfviaxs5li4wtl744fj4fi3q\"\n",
    "CONFIG_PROFILE = \"DEFAULT\"\n",
    "config = oci.config.from_file('config', CONFIG_PROFILE)\n",
    "\n",
    "# Service endpoint\n",
    "endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config, service_endpoint=endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "\n",
    "# Function to extract text from PDF (RFP)\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = ''\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from PPTX (RFP response)\n",
    "def extract_text_from_pptx(pptx_path):\n",
    "    prs = Presentation(pptx_path)\n",
    "    text = ''\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                text += shape.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Load and extract text from RFP and RFP Response\n",
    "rfp_text = extract_text_from_pdf('rfp.pdf')\n",
    "rfp_response_text = extract_text_from_pptx('rfp_response.pptx')\n",
    "\n",
    "# Concatenate the two documents for context\n",
    "combined_input = f\"RFP Document:\\n{rfp_text}\\n\\nRFP Response Document:\\n{rfp_response_text}\"\n",
    "\n",
    "# List of questions to ask (tailored for requirements gathering document)\n",
    "questions = [\n",
    "    \"What is the purpose and scope of this project as outlined in the RFP?\",\n",
    "    \"What are the main business objectives that this project aims to achieve for Abu Dhabi Aviation Holding?\",\n",
    "    \"What are the key functional requirements that the solution must fulfill, including features and modules needed for reporting and consolidation?\",\n",
    "    \"What are the key non-functional requirements, such as performance, scalability, availability, and maintainability of the system?\",\n",
    "    \"Can you describe the overall solution architecture, including the key components, integrations, and data flows involved in this project?\",\n",
    "    \"What are the specific integration requirements between Oracle FCCS and external systems like Great Plains, Rusada, and others?\",\n",
    "    \"What is the data management strategy, including data storage, retention, and backup policies for this project?\",\n",
    "    \"What are the data security protocols, including user access control, encryption methods, and audit logging mechanisms?\",\n",
    "    \"What are the automation and workflow requirements for processes like data consolidation, variance analysis, and reporting?\",\n",
    "    \"What are the specific reporting requirements, including the use of SmartView, dashboards, and other tools for ad-hoc reporting?\",\n",
    "    \"What are the assumptions and constraints identified for this project, including dependencies on other systems and limitations of the current environment?\",\n",
    "    \"What are the key deliverables and milestones for this project, including UAT, go-live support, and post-implementation support?\"\n",
    "]\n",
    "\n",
    "# Function to ask multiple questions and get responses\n",
    "def ask_multiple_questions(questions, input_text):\n",
    "    chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "    chat_responses = []\n",
    "    \n",
    "    for question in questions:\n",
    "        # Create chat request for each question\n",
    "        chat_request = oci.generative_ai_inference.models.CohereChatRequest()\n",
    "        chat_request.message = f\"{question}\\n\\nContext:\\n{input_text}\"\n",
    "        chat_request.max_tokens = 600\n",
    "        chat_request.temperature = 0.15  # Lower temperature for more technical responses\n",
    "        chat_request.frequency_penalty = 0\n",
    "        chat_request.top_p = 0.85  # Bias towards more likely words to maintain technical tone\n",
    "        chat_request.top_k = 0\n",
    "\n",
    "        # Set chat details and compartment info\n",
    "        chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=\"ocid1.generativeaimodel.oc1.us-chicago-1.amaaaaaask7dceya7ozidbukxwtun4ocm4ngco2jukoaht5mygpgr6gq2lgq\")\n",
    "        chat_detail.chat_request = chat_request\n",
    "        chat_detail.compartment_id = compartment_id\n",
    "\n",
    "        # Send request and get response\n",
    "        chat_response = generative_ai_inference_client.chat(chat_detail)\n",
    "\n",
    "        # Extract the message from the chat history\n",
    "        chat_history = chat_response.data.chat_response.chat_history\n",
    "        if chat_history:\n",
    "            response_message = chat_history[-1].message\n",
    "            chat_responses.append(f\"Question: {question}\\nResponse: {response_message}\\n\")\n",
    "\n",
    "    return chat_responses\n",
    "\n",
    "# Ask the list of questions\n",
    "responses = ask_multiple_questions(questions, combined_input)\n",
    "\n",
    "# Print each response\n",
    "for response in responses:\n",
    "    print(\"**************************Chat Result**************************\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "71b5aa1f-4ec7-4bda-939a-5295af4480e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File PROJECT_SPEAK_MATE/DELIVERY_DOCS/updated_requirements_document.docx uploaded to Object Storage in bucket ECHO.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import oci\n",
    "import io\n",
    "\n",
    "\n",
    "compartment_id = \"ocid1.compartment.oc1..aaaaaaaaretksgipt3jgwfpzgh4ijyw54uynyfviaxs5li4wtl744fj4fi3q\"\n",
    "config = oci.config.from_file('config', 'DEFAULT')\n",
    "object_storage_client = oci.object_storage.ObjectStorageClient(config)\n",
    "\n",
    "def extract_question_and_response(combined_text):\n",
    "    question_match = re.search(r'Question:\\s*(.+?)\\n', combined_text)\n",
    "    response_match = re.search(r'Response:\\s*(.+)', combined_text, re.S)\n",
    "    \n",
    "    if question_match and response_match:\n",
    "        question = question_match.group(1)\n",
    "        response = response_match.group(1).strip()\n",
    "        return question, response\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def extract_keywords(question):\n",
    "    keywords = re.findall(r'\\b(purpose and scope|business objectives|functional requirements|non-functional requirements|solution architecture|data management|security|reporting requirements|automation|key deliverables)\\b', question, re.I)\n",
    "    return ' '.join([kw.capitalize() for kw in keywords]) if keywords else 'General Section'\n",
    "\n",
    "\n",
    "project_title = \"Speak Mate\"\n",
    "for paragraph in doc.paragraphs:\n",
    "    if '<Project title>' in paragraph.text:\n",
    "        paragraph.text = paragraph.text.replace('<Project title>', project_title)\n",
    "\n",
    "\n",
    "def insert_after_page_3(doc, responses):\n",
    "    paragraphs = list(doc.paragraphs)\n",
    "    insert_after = None\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        if \"Customer Experience\" in paragraph.text:\n",
    "            insert_after = i + 1\n",
    "            break\n",
    "\n",
    "    if insert_after:\n",
    "        for response in responses:\n",
    "            question, answer = response.split('\\nResponse: ', 1)\n",
    "            heading = extract_keywords(question)\n",
    "            doc.paragraphs.insert(insert_after, doc.add_heading(heading, level=2))\n",
    "            doc.paragraphs.insert(insert_after + 1, doc.add_paragraph(answer))\n",
    "            insert_after += 2\n",
    "            \n",
    "# Ensure no overlap with footer by adjusting bottom margin\n",
    "def ensure_no_footer_overlap(doc):\n",
    "    for section in doc.sections:\n",
    "        section.bottom_margin = Inches(1)\n",
    "        section.top_margin = Inches(1)\n",
    "        section.left_margin = Inches(1)\n",
    "        section.right_margin = Inches(1)\n",
    "        \n",
    "        \n",
    "# Load the requirements template\n",
    "template_path = 'requirements_template.docx'\n",
    "doc = Document(template_path)\n",
    "\n",
    "# Insert responses between the designated pages\n",
    "insert_after_page_3(doc, responses)\n",
    "ensure_no_footer_overlap(doc)\n",
    "\n",
    "# Create a BytesIO stream to hold the document in memory\n",
    "doc_stream = io.BytesIO()\n",
    "doc.save(doc_stream)\n",
    "doc_stream.seek(0)\n",
    "\n",
    "# Upload the in-memory document to Object Storage\n",
    "def upload_to_object_storage_from_memory(bucket_name, object_name, doc_stream):\n",
    "    object_storage_client.put_object(\n",
    "        namespace_name=namespace_name,\n",
    "        bucket_name=bucket_name,\n",
    "        object_name=object_name,\n",
    "        put_object_body=doc_stream\n",
    "    )\n",
    "    print(f\"File {object_name} uploaded to Object Storage in bucket {bucket_name}.\")\n",
    "\n",
    "# Define your Object Storage bucket and namespace here\n",
    "bucket_name = \"ECHO\"\n",
    "namespace_name = \"gc35013\"\n",
    "folder_path = \"PROJECT_SPEAK_MATE/DELIVERY_DOCS\"\n",
    "\n",
    "object_name = f\"{folder_path}/updated_requirements_document.docx\"\n",
    "\n",
    "upload_to_object_storage_from_memory(bucket_name,object_name,doc_stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
