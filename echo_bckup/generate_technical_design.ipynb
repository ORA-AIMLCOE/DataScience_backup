{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fb43832-b23d-4df8-a28e-cb73e2a20ce3",
   "metadata": {},
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "199b8410-7575-4e15-90fb-9ac36499dfd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import oci\n",
    "import io\n",
    "from datetime import datetime, timedelta\n",
    "import docx\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "150776c7-b04c-4853-96d1-43e22cbc7bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compartment_id = \"ocid1.compartment.oc1..aaaaaaaaretksgipt3jgwfpzgh4ijyw54uynyfviaxs5li4wtl744fj4fi3q\"\n",
    "CONFIG_PROFILE = \"DEFAULT\"\n",
    "config = oci.config.from_file('config', CONFIG_PROFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a50ecb59-16ba-4de1-9a8a-9c368e97e5bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config, service_endpoint=endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cabb4dda-a59c-4e73-b0ac-d228c2126aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_question(question, input_text):\n",
    "    chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "    chat_request = oci.generative_ai_inference.models.CohereChatRequest()\n",
    "    \n",
    "    # Craft the input for the AI model\n",
    "    chat_request.message = f\"{question}\\n\\nContext:\\n{input_text}\"\n",
    "    chat_request.max_tokens = 1600\n",
    "    chat_request.temperature = 0.85  # Low temperature for precise responses\n",
    "    chat_request.frequency_penalty = 0\n",
    "    chat_request.top_p = 0.85  # Bias towards more likely words to maintain technical tone\n",
    "    chat_request.top_k = 0\n",
    "\n",
    "    chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=\"ocid1.generativeaimodel.oc1.us-chicago-1.amaaaaaask7dceya7ozidbukxwtun4ocm4ngco2jukoaht5mygpgr6gq2lgq\")\n",
    "    chat_detail.chat_request = chat_request\n",
    "    chat_detail.compartment_id = compartment_id\n",
    "\n",
    "    chat_response = generative_ai_inference_client.chat(chat_detail)\n",
    "    chat_history = chat_response.data.chat_response.chat_history\n",
    "    if chat_history:\n",
    "        return chat_history[-1].message\n",
    "    return \"No response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849448c7-f503-4d0e-8281-0714743934de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = ''\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Extract text from PPTX (using python-pptx)\n",
    "from pptx import Presentation\n",
    "\n",
    "def extract_text_from_pptx(pptx_path):\n",
    "    prs = Presentation(pptx_path)\n",
    "    text = ''\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                text += shape.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_requirements_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "771f3e50-f606-441f-b9ab-f4dc685bf399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load RFP and RFP Response\n",
    "rfp_text = extract_text_from_pdf('rfp.pdf')\n",
    "rfp_response_text = extract_text_from_pptx('rfp_response.pptx')\n",
    "requirements_text = extract_text_from_requirements_docx('PROJECT_SPEAK_MATE_DELIVERY_DOCS_updated_requirements_document (4).docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6770f98e-cc38-4de1-afb6-1baabd475a2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the purpose and scope of the project based on the RFP and response?\",\n",
    "    \"What are the key technical terms and abbreviations used in the project?\",\n",
    "    \"What are the key assumptions for this project?\",\n",
    "    \"What documents, systems, or standards are referenced for this project?\",\n",
    "    \"What is the overall solution architecture, including workflows and key processes?\",\n",
    "    \"What is the physical setup for this solution?\",\n",
    "    \"What is the data modeling and migration strategy for this project?\",\n",
    "    \"What are the key custom UI screens and logic details?\",\n",
    "    \"What authentication mechanisms are being used for this project?\",\n",
    "    \"What are the key integration points for this project?\",\n",
    "    \"What reports are required for this project, and how will they be generated?\",\n",
    "    \"How will errors be handled in the integration and application layers?\",\n",
    "    \"What is the notification mechanism for the system?\",\n",
    "    \"What are the key database objects being used for this project?\",\n",
    "    \"What are the main REST services and APIs used for communication?\",\n",
    "    \"What are the post-implementation support activities planned for this project?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20ca53ab-10f6-4901-9590-e476a7f2a31a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d31e387-9607-44a4-8ee9-104a9c02d6b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_technical_spec(rfp_text, rfp_response_text, requirements_text):\n",
    "    # Create a dictionary to hold each section of the technical specification\n",
    "    spec_document = {}\n",
    "    \n",
    "    # Map each question to the relevant section of the document\n",
    "    spec_document['Purpose and Scope'] = ask_question(questions[0], rfp_text)\n",
    "    spec_document['Glossary of Technical Terms'] = ask_question(questions[1], rfp_response_text)\n",
    "    spec_document['Assumptions'] = ask_question(questions[2], requirements_text)\n",
    "    spec_document['References'] = ask_question(questions[3], rfp_text)\n",
    "    \n",
    "    # Solution Overview\n",
    "    spec_document['Solution Architecture'] = ask_question(questions[4], rfp_response_text)\n",
    "    \n",
    "    # Application Components\n",
    "    spec_document['Physical Setup'] = ask_question(questions[5], requirements_text)\n",
    "    spec_document['Data Modeling and Migration'] = ask_question(questions[6], rfp_response_text)\n",
    "    spec_document['Custom UI Screens and Logic'] = ask_question(questions[7], requirements_text)\n",
    "    spec_document['Authentication Mechanisms'] = ask_question(questions[8], rfp_response_text)\n",
    "    \n",
    "    # Services Integration\n",
    "    spec_document['Integration Points'] = ask_question(questions[9], rfp_response_text)\n",
    "    spec_document['Reports and Dashboards'] = ask_question(questions[10], requirements_text)\n",
    "    \n",
    "    # Error Handling and Notifications\n",
    "    spec_document['Error Handling'] = ask_question(questions[11], rfp_response_text)\n",
    "    spec_document['Notifications'] = ask_question(questions[12], requirements_text)\n",
    "    \n",
    "    # Database and APIs\n",
    "    spec_document['Database Objects'] = ask_question(questions[13], requirements_text)\n",
    "    spec_document['REST Services and APIs'] = ask_question(questions[14], rfp_response_text)\n",
    "    \n",
    "    # Post-Implementation Support\n",
    "    spec_document['Post-Implementation Support'] = ask_question(questions[15], requirements_text)\n",
    "    \n",
    "    return spec_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19519709-87a3-4e21-93e2-9a47cb1f773a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "technical_spec = generate_technical_spec(rfp_text, rfp_response_text, requirements_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de008956-3521-47d5-9d69-b926467e4529",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Purpose and Scope': 'The purpose of the project is to implement a consolidation and reporting solution for Abu Dhabi Aviation Holding, leveraging the existing ADQ Aviation application and architecture. The scope of the project includes:\\n\\n- Enabling consolidation for statutory and management reporting\\n- Implementing the ADA entity and ownership structure\\n- Automating data integrations and loads for all entities\\n- Enabling SmartView-based reporting packs\\n- Facilitating data movements from ADA to AAS for statutory and management reporting\\n- Enabling consolidation of actual, management, budget, and forecast scenarios\\n- Providing variance analysis capabilities\\n- Configuring standard out-of-box ratios and KPIs, as well as additional ratios for operational reporting\\n\\nThe project aims to facilitate monthly reporting, data movements, and consolidations for ADA, adhering to ADQ Aviation reporting requirements.',\n",
       " 'Glossary of Technical Terms': \"Here is a list of key technical terms and abbreviations used in the project:\\n\\n- LLM: Large Language Model, a type of machine learning model that can process and generate human-like language.\\n- Oracle Data Mart: A data warehouse solution provided by Oracle that enables organizations to store, manage, and analyze data from various sources.\\n- Oracle Analytics: Oracle's suite of business intelligence and data visualization tools, including dashboards and reports.\\n- OCI: Oracle Cloud Infrastructure, a set of cloud services provided by Oracle for building and running applications in the cloud.\\n- ODA: Oracle Digital Assistant, a platform for building and deploying chatbots and other conversational interfaces.\\n- API: Application Programming Interface, a set of defined rules that allow different software applications to communicate with each other.\\n- GPU: Graphics Processing Unit, a specialized electronic circuit designed to rapidly process large amounts of data and accelerate computational tasks.\\n- RBAC: Role-Based Access Control, a security mechanism that restricts access to authorized users based on their roles and responsibilities.\\n- SQL: Structured Query Language, a programming language used to interact with relational databases.\\n- STT: Speech-to-Text, a technology that converts spoken language into written text.\\n- TTS: Text-to-Speech, a technology that converts written text into spoken language.\\n- UAT: User Acceptance Testing, a process of verifying that a solution works correctly and meets the user's requirements.\\n- FCCS: Oracle Enterprise Performance Reporting Cloud Service, a platform for financial reporting and analysis.\\n- ODI: Oracle Data Integrator, a tool used for data integration and transformation.\\n- ADW: Autonomous Data Warehouse, a cloud-based data warehousing service provided by Oracle.\\n- OAC: Oracle Analytics Cloud, a platform for data analysis and visualization.\\n- AIMS: Aircraft Information Management System, a software system used for managing aircraft and crew data.\\n- IAM/IDCS: Identity and Access Management/Identity Cloud Service, a security platform for managing user identities and access to resources.\\n- APEX: Application Express, a development platform for building web applications on Oracle databases.\\n- OCR: Optical Character Recognition, a technology that extracts text from images or scanned documents.\\n- PPS: Product Planning and Control System\\n- AV: Avatar\\n- AD: Active Directory\\n\\nThese terms and abbreviations are specific to the field of data analytics, cloud computing, and machine learning, and are essential for understanding the project's scope and technical details.\",\n",
       " 'Assumptions': \"The key assumptions for this project are:\\n- Utilization of Oracle's Cloud Infrastructure (OCI) services in both Non-Prod and Prod environments.\\n- Development of a Data Mart for FCCS and additional data sources, with specified file extracts, attributes, subject areas, and reports.\\n- LLM training and integration, including testing, fine-tuning, and prompt engineering.\\n- Enabling and integrating Oracle Digital Assistant (ODA) with the selected LLM.\\n- Integration of an avatar system with text-to-speech and speech-to-text capabilities, following ADA branding guidelines.\\n- Conducting train-the-trainer sessions for knowledge transfer.\\n- Providing UAT and go-live support for a specified duration.\\n- Adaptability of the implementation efforts and timelines based on project requirements and complexities.\\n- Data quality confirmation and data cleansing to be handled by ADA.\\n- Collaboration with a core team from ADA for functional and technical inputs.\\n- Flexibility in LLM selection, with accuracy parameters jointly reviewed by ADA and Oracle.\\n- Customization of the avatar system according to ADA's branding guidelines.\\n- Prioritization of OCI native services over other technologies.\\n- Availability of a dedicated project manager from ADA.\\n- Responsibility of organizational change management and user training assigned to ADA.\\n- Full access to relevant resources and skilled personnel ensured by ADA.\\n- Coordination with vendors for network connectivity by ADA.\\n- Handling of 3rd party/configuration changes, further maintenance, and data availability by ADA.\\n\\nThese assumptions provide a foundation for the project and are subject to review and adjustment as the project progresses.\",\n",
       " 'References': 'The following documents, systems, and standards are referenced in the provided text:\\n\\n- Oracle EPM Cloud FCCS\\n- UCOA (Uniform Chart of Accounts)\\n- AED (local/functional currency)\\n- GAAP (Generally Accepted Accounting Principles)\\n- SmartView\\n- KPI (Key Performance Indicators)',\n",
       " 'Solution Architecture': \"The overall solution architecture for the proposed Oracle Data Mart & Analytics project with an LLM-powered Oracle Chatbot as a smart chat assistant involves a combination of Oracle Cloud Infrastructure (OCI) services and third-party technologies. \\n\\n**Workflows:**\\n\\n1. **Data Ingestion and Preparation:** \\n   - Data sources include Oracle FCCS, Procurement, HR, and systems like Great Plains, Rusada, Adrenalin, and Jaggaer Procurement for entities ADA, EYE, AMMROC, and GAL. \\n   - Data extraction is performed using SQL queries executed by ADA on the source systems, with files provided in an OCI object store. \\n   - Data quality and cleansing are managed by ADA, ensuring clean and consistent data for the project. \\n\\n2. **Data Mart Development:** \\n   - A data mart is designed and developed to store and organize data from various sources. \\n   - It includes subject areas (simple, medium, and complex) and reports based on the complexity and requirements. \\n\\n3. **LLM Model Training and Integration:** \\n   - LLM models are trained using Gen AI APIs to generate SQL queries and understand human-like text. \\n   - Fine-tuning and prompt engineering are performed to enhance the LLM's performance. \\n   - The LLM is integrated with the Oracle Digital Assistant (ODA) platform to enable conversational capabilities. \\n\\n4. **Avatar System Integration:** \\n   - A text-to-speech (TTS) and speech-to-text (STT) avatar system is integrated with the LLM to provide a user-friendly interface. \\n\\n5. **Oracle Analytics Cloud (OAC):** \\n   - OAC is utilized to create insightful dashboards and reports, providing powerful analytics capabilities. \\n\\n6. **Security and Access Management:** \\n   - Oracle Identity Access Management Cloud Services (IAM/IDCS) is used for secure user authentication and access control. \\n\\n7. **Development and Testing:** \\n   - Development environments are set up for testing and UAT, with subsequent deployment to production. \\n\\n8. **Training and Support:** \\n   - Train-the-trainer sessions are conducted to ensure effective knowledge transfer. \\n   - UAT and go-live support are provided for a smooth transition. \\n\\n**Key Processes:**\\n\\n1. **Data Mart and LLM Powered Oracle Chat:** \\n   - This process involves setting up the OCI foundation, data integration, autonomous database, ODA, object storage, and other required OCI services. \\n\\n2. **Environment Setup:** \\n   - Non-prod and prod environments are established, including OCI foundation setup, data integration, database configuration, ODA setup, and avatar system integration. \\n\\n3. **Data Sources and Extensions:** \\n   - Data sources are identified and integrated, currently including Oracle FCCS, Procurement, and HR for the specified entities. \\n\\n4. **Data Mart Development:** \\n   - Subject areas and reports are designed and developed based on the data sources and business requirements. \\n\\n5. **LLM Training and Integration:** \\n   - LLM models are trained and fine-tuned to understand and generate human-like text, with integration into the ODA platform. \\n\\n6. **Avatar System Integration:** \\n   - The avatar system, with TTS and STT capabilities, is integrated with the LLM to provide a conversational interface for users. \\n\\n7. **Oracle Analytics Cloud:** \\n   - OAC is utilized to create dashboards and reports, leveraging the data in the data mart. \\n\\n8. **Security and Access Control:** \\n   - IAM/IDCS is configured to manage user access and authentication, ensuring secure access to the platform. \\n\\nThe solution architecture focuses on integrating advanced conversational AI capabilities with a robust data mart and analytics platform, providing a seamless and intuitive user experience for data interaction and analysis.\",\n",
       " 'Physical Setup': 'Apologies, but I was unable to find specific details regarding the physical setup for the solution outlined in the provided text. However, based on the context, I can infer that the solution involves a combination of hardware, software, and network infrastructure components. \\n\\nHere is a high-level overview of the physical setup based on the information provided: \\n\\n- **Hardware**: \\n   - Servers: The solution likely requires servers to host the various components, including the Oracle Cloud Infrastructure (OCI) services, Data Mart, LLM-powered Oracle Chatbot, and Oracle Analytics Cloud (OAC). These servers need to be set up and configured to handle the expected workload and ensure high availability.\\n   - Storage: A robust storage infrastructure is necessary to accommodate the Data Mart, which can store up to 1 TB of data in the production environment and 100 GB in the non-production environment. This includes setting up secure and scalable storage systems, such as network-attached storage (NAS) or storage area networks (SAN).\\n   - Networking Equipment: Physical networking devices such as routers, switches, and firewalls are required to connect the different components and ensure secure data transmission.\\n\\n- **Software**: \\n   - Oracle Cloud Infrastructure (OCI): OCI provides the underlying infrastructure and services, including compute, storage, networking, and database services. This involves setting up and configuring the necessary OCI services, such as Oracle Data Integration, Autonomous Database, Oracle Digital Assistant, and Object Storage.\\n   - Oracle Data Mart: The Data Mart serves as the central repository for data aggregation and storage. It needs to be set up and integrated with the various data sources, including FCCS, Procurement, HR, and other systems.\\n   - LLM-Powered Oracle Chatbot: The chatbot is powered by Large Language Model technology, providing advanced conversational capabilities. This involves training and integrating the LLM models with the Oracle Digital Assistant (ODA) and avatar system.\\n   - Oracle Analytics Cloud (OAC): OAC offers analytics and data visualization capabilities. It needs to be set up and integrated with the Data Mart to access the centralized data repository.\\n\\n- **Network Infrastructure**: \\n   - Connectivity: Ensure reliable and secure network connectivity between the different components, including servers, storage systems, and user endpoints. This may involve configuring local area networks (LANs), wide area networks (WANs), and virtual private networks (VPNs).\\n   - Security: Implement security measures such as firewalls, intrusion detection systems, and virtual private networks to protect sensitive data during transmission and storage.\\n   - Performance Optimization: Optimize network routing and bandwidth allocation to ensure efficient data transfer and response times, especially when handling large data volumes.\\n\\nThis solution likely involves a combination of on-premises hardware and cloud-based infrastructure, leveraging the capabilities of Oracle Cloud Infrastructure. The physical setup would vary depending on the specific requirements and scale of the organization. \\n\\nPlease note that this is a high-level overview, and the specific physical setup may include additional components or variations based on the detailed design and requirements of the project.',\n",
       " 'Data Modeling and Migration': \"A data modeling and migration strategy is a plan that outlines the steps involved in designing and implementing a data model, as well as migrating data from one system to another. Here is a suggested strategy based on the provided context:\\n\\n## Data Modeling Strategy:\\n1. Identify Business Requirements: Work closely with stakeholders from Abu Dhabi Aviation to understand their specific data needs and requirements for the Oracle Data Mart and Analytics platform.\\n\\n2. Data Analysis: Perform a comprehensive analysis of the data sources mentioned, including Oracle FCCS, Procurement, HR, Great Plains, Rusada, Adrenalin, and Jaggaer Procurement. Understand the data structures, relationships, and attributes of each source.\\n\\n3. Conceptual Data Model: Develop a high-level conceptual data model that represents the overall structure and entities of the data mart. This model should align with the business requirements and provide a logical organization of the data.\\n\\n4. Logical Data Model: Translate the conceptual data model into a logical data model, defining the specific tables, attributes, relationships, and data types. Consider data normalization techniques to minimize redundancy and improve data integrity.\\n\\n5. Physical Data Model: Design the physical data model that reflects the target database management system (DBMS) and takes into account performance, indexing, and storage considerations. Make sure to utilize Oracle-specific features and best practices for optimal performance.\\n\\n6. Data Validation: Validate the data model by performing data quality checks, ensuring that the model can accommodate the required data and support the intended queries and analytics.\\n\\n7. Documentation: Document the data model thoroughly, including entity-relationship diagrams, table structures, data dictionaries, and any relevant metadata. This documentation will serve as a reference for future development and maintenance.\\n\\n## Data Migration Strategy:\\n1. Data Extraction: Extract data from the source systems mentioned, ensuring that the data is cleansed, transformed, and formatted according to the requirements of the target data mart. Use appropriate tools and scripts to automate the extraction process.\\n\\n2. Data Transformation: Apply transformation rules to map the source data to the target data model. This may involve data cleansing, deduplication, and converting data formats to match the target system's requirements.\\n\\n3. Data Load: Load the transformed data into the target data mart, utilizing Oracle's data loading tools and best practices. Consider strategies such as bulk loading or incremental loading, depending on the volume and nature of the data.\\n\\n4. Data Validation: Perform extensive data validation to ensure that the migrated data is accurate, complete, and consistent. Compare the data in the source and target systems, validating key metrics and performing data quality checks.\\n\\n5. Cutover and Testing: Plan a cutover strategy to minimize downtime during the migration. Conduct comprehensive testing, including unit testing, integration testing, and user acceptance testing (UAT), to verify that the migrated data is functioning correctly in the new environment.\\n\\n6. Post-Migration Support: Provide post-migration support to address any data-related issues that may arise. Monitor the performance and usage of the data mart, and be prepared to make adjustments as needed.\\n\\n7. Documentation: Document the entire migration process, including the data sources, transformation rules, loading procedures, and validation results. This documentation will be valuable for future reference and audit purposes.\\n\\nOverall, the data modeling and migration strategy aims to design a robust data model that meets the business requirements and seamlessly integrate data from various sources into the Oracle Data Mart and Analytics platform, enabling advanced conversational AI capabilities and powerful analytics.\",\n",
       " 'Custom UI Screens and Logic': 'I apologize, but I could not find any mention of custom UI screens or logic details in the provided text. If this information is present elsewhere, please provide additional context or clarify your request so that I can better assist you.',\n",
       " 'Authentication Mechanisms': \"I'm sorry, but I was unable to find any specific mention of authentication mechanisms or security protocols within the provided context. However, it seems that the project involves the implementation of an Oracle chatbot integrated with a Data Mart and various other Oracle cloud services.\\n\\nIf we consider standard industry practices and the sensitivity of data involved (especially with HR and financial data sources mentioned), it is likely that robust authentication and authorization mechanisms are in place. This could include password policies, multi-factor authentication, role-based access controls, and encryption protocols.\\n\\nIt would be best to refer to the project's technical documentation or consult with the project's IT team or security experts for specific details on the authentication mechanisms employed.\",\n",
       " 'Integration Points': \"Based on the provided context, the key integration points for this project appear to be:\\n\\n1. Oracle Chatbot and Large Language Model (LLM) Integration: The project involves integrating a state-of-the-art LLM with the Oracle chatbot to enable advanced conversational capabilities. This integration will allow the chatbot to understand and generate human-like text, providing sophisticated and nuanced responses to user queries.\\n\\n2. Oracle Data Mart and Chatbot Integration: The Oracle chatbot will be seamlessly integrated with Oracle's Data Mart, providing real-time access to data. This integration will enable users to query data, receive responses, and perform data-related tasks through simple conversational interactions with the chatbot.\\n\\n3. Oracle Analytics and Chatbot Integration: Oracle Analytics cloud will be integrated with the chatbot to provide insightful dashboards and reports. This integration will allow users to visualize and interpret data more effectively, making informed decisions based on the information retrieved through the chatbot.\\n\\n4. Avatar System Integration: The project also includes integrating an avatar system with text-to-speech (TTS) and speech-to-text (STT) models. This avatar will be linked with the LLM, enabling a more interactive and engaging user experience.\\n\\n5. Oracle Digital Assistant (ODA) and LLM Integration: Enabling and integrating ODA with the LLM will facilitate a seamless conversational interface for users to interact with the underlying data and analytics platform.\\n\\n6. Data Sources and Extensions: Currently, data sources include Oracle FCCS, Procurement, and HR for 4 entities (ADA, EYE, AMMROC, and GAL). Integrating these data sources with the Data Mart and making them accessible through the chatbot is a key aspect of the project.\\n\\n7. Oracle Cloud Infrastructure (OCI) Services: The project will leverage various OCI services, such as Object Storage, Oracle Identity Access Management Cloud Services, Oracle Analytics Cloud, Oracle API Gateway, OCI GPU for LLM, and OKE, to support the overall implementation.\\n\\n8. Third-Party Integrations: Licenses and integrations with third-party systems like Adrenalin, Great Plains, Rusada, AIMS, and Jagger eProcurement will be required to ensure data availability and accessibility for the Data Mart and chatbot.\",\n",
       " 'Reports and Dashboards': \"Based on the provided context, here are the reports that are required for the project and the process for generating them:\\n\\n## Reporting Requirements:\\n- **Statutory Reporting**:\\n   - **Financial Statements**: These reports include income statements, balance sheets, and cash flow statements. They are typically generated on a periodic basis, such as monthly, quarterly, or annually, to comply with regulatory requirements.\\n   - **Management Reports**: Management reports provide insights to internal stakeholders, such as executives and department heads. They may include variance analysis, budget vs. actual comparisons, and key performance indicators (KPIs).\\n\\n- **Ad-Hoc Reporting**:\\n   - **SmartView Reporting Packs**: SmartView is a tool that enables users to create ad-hoc reports and perform analysis directly on the FCCS application. It will be used to generate reporting packs for statutory purposes, allowing users to access and explore data interactively.\\n\\n- **Dashboards and Visualizations**:\\n   - **Close Management Dashboard**: This dashboard will provide an overview of the financial close process, including the status of data integrations, validation checks, and any issues identified.\\n   - **Compliance Dashboard**: The compliance dashboard will monitor key compliance metrics and provide visibility into regulatory requirements, ensuring adherence to financial reporting standards.\\n   - **Financial Insights Dashboard**: Offering financial KPIs, trends, and metrics to support decision-making, this dashboard will provide a visual representation of financial performance.\\n\\n- **Oracle Data Mart and Analytics Platform**:\\n   - **Data Mart Reports**: Using data from various sources, including FCCS, Procurement, and HR systems, the Data Mart will generate reports. These reports will be tailored to the specific needs of the business and may include operational, financial, and analytical insights.\\n   - **Oracle Analytics Cloud (OAC) Reports and Dashboards**: OAC will be leveraged to create insightful dashboards and reports. It provides advanced analytics capabilities, enabling users to explore data, identify patterns, and make data-driven decisions.\\n\\n## Generation Process:\\n- **Data Collection and Integration**: Data will be extracted from various source systems, including FCCS, Procurement, HR, and other third-party applications. Data integration processes will be automated to ensure seamless data flow into the Data Mart.\\n- **Data Validation and Transformation**: Once the data is collected, validation checks will be performed to ensure data accuracy and consistency. Data transformation techniques will be applied to standardize and format the data for reporting.\\n- **Report Generation**: Utilizing tools like SmartView, OAC, and the Data Mart's reporting capabilities, reports will be generated. Standard reports will be automated, and ad-hoc reports can be created as needed.\\n- **Distribution and Access**: Reports will be distributed to relevant stakeholders securely. Access controls will be implemented to ensure that users only view reports relevant to their roles and responsibilities.\\n- **Workflow and Collaboration**: A well-defined workflow will be established for report approval, review, and distribution. Collaboration tools will be leveraged to facilitate communication and coordination between teams during the reporting process.\\n- **Alerting and Notification**: Automated alerts and notifications will be set up to notify stakeholders of report availability and any critical issues or variances identified in the data.\\n\\nThis comprehensive approach to reporting ensures that the project meets its statutory, management, and ad-hoc reporting requirements, providing valuable insights and supporting data-driven decision-making.\",\n",
       " 'Error Handling': \"Error handling in the integration and application layers is essential to ensure the smooth functioning of the system and to provide a seamless user experience. Here are some approaches to handle errors in these layers:\\n\\n1. Exception Handling: Implement robust exception handling mechanisms in the integration and application layers. Catch exceptions that occur during data exchange, API calls, or any other integration points. Log the exceptions, provide meaningful error messages to users, and gracefully handle failures to prevent system crashes.\\n\\n2. Error Logging and Monitoring: Set up comprehensive error logging and monitoring systems. Log errors and exceptions along with relevant details such as timestamps, error types, and stack traces. Use logging frameworks or tools that allow for centralized logging and easy error analysis. Monitor the logs regularly to identify recurring issues and potential system bottlenecks.\\n\\n3. Retry and Fallback Mechanisms: Implement retry mechanisms for failed operations. For example, if an API call fails due to temporary network issues, retrying the request after a short delay may lead to success. Additionally, consider implementing fallback mechanisms for critical functions. For instance, if an external service is unavailable, the system could provide a default response or redirect the user to an alternative functionality.\\n\\n4. Data Validation: Validate data at the integration points to ensure it meets the expected format and constraints. Implement data validation rules to check for data integrity, consistency, and security. Reject or handle invalid data gracefully to prevent data-related errors from propagating through the system.\\n\\n5. Error Handling Policies: Define clear error handling policies and guidelines for the integration and application layers. Establish standard error codes, error messages, and error escalation procedures. Ensure that error handling is consistent across the system and aligns with industry best practices.\\n\\n6. Testing and Simulation: Conduct thorough testing, including integration testing, to identify and address potential errors before deployment. Simulate different failure scenarios, such as network failures, service unavailability, or data inconsistencies, to assess the system's resilience and error handling capabilities.\\n\\n7. Rollback and Recovery: Implement rollback mechanisms for failed transactions or operations to maintain data consistency. In case of errors, the system should be able to undo or compensate for the changes made up to the point of failure. Additionally, establish recovery procedures to restore the system to a stable state after a failure.\\n\\n8. Error Notification and User Feedback: Provide users with clear and timely error notifications. Display user-friendly error messages that explain the issue and, if possible, suggest corrective actions. Consider implementing feedback loops where users can report issues or provide feedback on the system's error handling.\\n\\n9. Continuous Improvement: Regularly review error logs, user feedback, and system performance metrics to identify recurring issues or areas for improvement. Use this information to refine the error handling mechanisms, enhance system resilience, and improve the overall user experience.\\n\\n10. Redundancy and Fault Tolerance: Introduce redundancy and fault tolerance in the system architecture. This could include load balancing, failover mechanisms, or distributed systems designs. By having multiple instances or backup options, the system can handle failures more gracefully and maintain availability.\\n\\nBy incorporating these approaches into the integration and application layers, errors can be effectively handled, minimizing disruptions and providing a positive user experience. It is important to tailor these strategies to the specific requirements and constraints of the Oracle Data Mart & Analytics system and to continuously monitor and improve error handling based on real-world usage.\",\n",
       " 'Notifications': \"I'm sorry, but the provided text does not seem to explicitly mention the notification mechanism for the system. However, I can provide you with some related information regarding reporting and communication within the project context. \\n\\nThe project involves implementing an advanced reporting and analytics solution for Abu Dhabi Aviation Holding, leveraging various tools and technologies. The reporting requirements mention the use of SmartView for ad-hoc reporting and statutory purposes. Additionally, there is a plan to configure out-of-the-box dashboards for close management and compliance, as well as building a financial dashboard. These dashboards will likely provide visualizations and key performance indicators to help users monitor different aspects of the business. \\n\\nThe project also includes the integration of a Large Language Model (LLM)-powered Oracle chatbot, which enhances the user experience for data interactions and analysis. This chatbot is expected to provide advanced conversational capabilities, allowing users to interact with data through natural language queries. It is integrated with the Data Mart, enabling real-time data access and interactions. \\n\\nWhile the provided text does not explicitly mention notification mechanisms, the use of dashboards, reports, and the interactive chatbot suggests that users will be able to receive updates, alerts, and notifications through these channels. The system may provide notifications within the dashboard interfaces, generate reports with relevant updates, or utilize the chatbot to communicate important information to users. \\n\\nIf there are specific requirements or details regarding notification mechanisms, they might be outlined in sections of the project documentation that were not provided in your question. You may need to refer to additional documents or sections within the provided text to find more specific information about notification mechanisms, alert settings, or communication protocols.\",\n",
       " 'Database Objects': \"The key database objects being used for this project are:\\n\\n1. Data Mart: This is a central repository for data from various sources, including FCCS, Procurement, and HR systems. It serves as the backbone of the project, providing a structured and organized view of the data for analysis and reporting. The Data Mart is designed to handle up to 1 TB of data in production and 100 GB in the non-production environment.\\n\\n2. FCCS (Oracle Enterprise Performance Management Cloud Financial Consolidation and Close): FCCS is a source system that provides financial data for consolidation and reporting. Data is extracted from FCCS and loaded into the Data Mart for further analysis.\\n\\n3. Procurement, Inventory, and HR Systems: Data is integrated from various operational systems, such as Great Plains, Rusada, Adrenalin, Jaggaer Procurement, and AIMS. These systems provide additional data points that are relevant for the project's reporting and analysis requirements.\\n\\n4. Oracle Analytics Cloud (OAC): OAC is utilized for creating dashboards, reports, and data models. It integrates with the Data Mart to access the centralized data and provides powerful analytics capabilities.\\n\\n5. Large Language Model (LLM)-powered Oracle Chatbot: The chatbot is a key component that leverages advanced conversational AI technology. It integrates seamlessly with the Data Mart, enabling users to interact with the data through natural language queries and perform data-related tasks.\\n\\n6. Oracle Digital Assistant (ODA): ODA is integrated with the LLM to provide a user-friendly chat interface. It serves as the front-end for users to interact with the chatbot and access the underlying data and analytics capabilities.\\n\\n7. Avatar System: The avatar system adds text-to-speech and speech-to-text capabilities to the chatbot, enhancing the user experience. It integrates with the LLM-powered Oracle Chatbot to provide a more human-like and engaging interface.\\n\\n8. Security and Access Control: Role-Based Access Control (RBAC), Multi-Factor Authentication (MFA), and Privileged Access Management (PAM) are implemented to ensure secure and controlled access to the data. Encryption methods are also employed to protect data at rest and in transit.\\n\\n9. Reporting Tools: SmartView is used for ad-hoc reporting and generating reporting packs for statutory purposes. Additionally, dashboards and reports are created using OAC to support decision-making and provide insights.\\n\\nThese database objects form the foundation of the project, enabling data consolidation, analysis, reporting, and advanced conversational capabilities. They work together to provide a comprehensive solution that meets the business objectives and requirements outlined in the project scope.\",\n",
       " 'REST Services and APIs': 'The main REST services and APIs used for communication in the context of the Oracle Data Mart and Analytics project are:\\n\\n- Oracle API Gateway: This provides a secure, scalable, and managed way to expose REST APIs and control access to back-end services. It allows for API monetization, analytics, and security policies to be applied to incoming requests.\\n- Oracle Cloud Infrastructure (OCI) services: OCI provides a range of cloud-based services, including compute, storage, networking, and database, which can be accessed and managed via REST APIs.\\n- Oracle Digital Assistant (ODA): ODA is a platform for building and deploying chatbots, and it integrates with various Oracle and third-party applications and services via REST APIs.\\n- Oracle Analytics Cloud (OAC): OAC offers a suite of business intelligence and analytics tools, and it exposes REST APIs for data retrieval, report generation, and dashboard manipulation.\\n- Oracle Autonomous Database: This is a cloud-based, self-driving, and self-securing database service that uses REST APIs for data access and management.\\n- Oracle Identity Access Management Cloud Services (IDCS): IDCS provides identity and access management capabilities, including user authentication, authorization, and single sign-on, via REST APIs.\\n\\nThese REST services and APIs facilitate communication between the various components of the Oracle Data Mart and Analytics solution, enabling seamless data integration, analysis, and interaction through the chatbot interface.',\n",
       " 'Post-Implementation Support': \"The post-implementation support activities planned for this project include:\\n\\n- **Post-Implementation Support:** As mentioned in the key deliverables and milestones section, ongoing post-implementation support is offered to address any issues that may arise and ensure the long-term stability of the solution. This support aims to provide assistance and guidance to address any challenges or problems that may occur after the system is live and in use.\\n\\n- **Training and Knowledge Transfer:** The project includes train-the-trainer sessions, implying that there will be knowledge transfer to enable internal trainers to educate other users effectively. This empowers the organization to become self-sufficient in using the new system.\\n\\n- **Maintenance and Enhancements:** ADA will be responsible for further maintenance activities, including backup procedures, monitoring, and housekeeping. This indicates a commitment to ongoing maintenance and system enhancements to ensure optimal performance and keep the solution up-to-date.\\n\\n- **Issue Resolution and Troubleshooting:** Post-implementation support typically involves addressing any issues or bugs that may surface after the system goes live. This could include troubleshooting, providing workarounds, or implementing fixes to ensure the system operates as intended.\\n\\n- **Performance Monitoring and Optimization:** Support activities may also encompass performance monitoring to ensure the system meets the defined performance requirements. This includes scalability, availability, and response time metrics. Optimization activities may be undertaken to improve the system's efficiency and effectiveness.\\n\\n- **User Feedback and Continuous Improvement:** Post-implementation support often involves collecting user feedback and making continuous improvements to the system. This may include incorporating user suggestions, addressing pain points, and enhancing the overall user experience based on feedback.\\n\\n- **Vendor Coordination:** Coordination with vendors, such as Oracle in this case, may be required for ongoing support and maintenance. This could include accessing their expertise, receiving updates or patches, and collaborating on resolving complex issues.\\n\\n- **Documentation and User Guides:** Creating and maintaining comprehensive documentation, user guides, and knowledge bases is an important aspect of post-implementation support. This ensures that users have access to up-to-date resources and can easily refer to instructions or troubleshooting steps.\\n\\nThese post-implementation support activities aim to provide a smooth transition to the new system, address any issues that arise, and ensure the long-term success and stability of the solution. It is important to define service level agreements (SLAs) and establish clear communication channels for effective post-implementation support.\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "technical_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a44aeff1-4543-491d-b17f-44a9f2d09a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(technical_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2a8abd2-0a49-44be-91db-95427dcfc03b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading PROJECT_SPEAK_MATE/DELIVERY_DOCS/technical_specification.docx to bucket ECHO...\n",
      "Upload complete: 200\n"
     ]
    }
   ],
   "source": [
    "def add_heading(doc, text, level):\n",
    "    doc.add_heading(text, level=level)\n",
    "\n",
    "def add_paragraph(doc, text):\n",
    "    doc.add_paragraph(text)\n",
    "    para.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "\n",
    "def generate_technical_spec_template(response_dict, template_path):\n",
    "    doc = Document(template_path)\n",
    "    doc.add_heading('1.Overview', level=1)\n",
    "    doc.add_heading('Purpose & Scope', level=2)\n",
    "    doc.add_paragraph(response_dict.get('Purpose and Scope', 'No data available'))\n",
    "\n",
    "    doc.add_heading('Glossary of Technical Terms', level=2)\n",
    "    doc.add_paragraph(response_dict.get('Glossary of Technical Terms', 'No data available'))\n",
    "\n",
    "    doc.add_heading('Assumptions', level=2)\n",
    "    doc.add_paragraph(response_dict.get('Assumptions', 'No data available'))\n",
    "\n",
    "    doc.add_heading('References', level=2)\n",
    "    doc.add_paragraph(response_dict.get('References', 'No data available'))\n",
    "\n",
    "    doc.add_heading('2.Solution Overview', level=1)\n",
    "    doc.add_paragraph(response_dict.get('Solution Architecture', 'No data available'))\n",
    "\n",
    "    doc.add_heading('3.Solution Components', level=1)\n",
    "    doc.add_heading('Physical View', level=2)\n",
    "    doc.add_paragraph(response_dict.get('Physical Setup', 'No data available'))\n",
    "\n",
    "    doc.add_heading('Data Modelling and Migration', level=2)\n",
    "    doc.add_paragraph(response_dict.get('Data Modeling and Migration', 'No data available'))\n",
    "\n",
    "    doc.add_heading('Custom UI Screens and Logic', level=2)\n",
    "    doc.add_paragraph(response_dict.get('Custom UI Screens and Logic', 'No data available'))\n",
    "\n",
    "    doc.add_heading('Authentication Mechanism', level=2)\n",
    "    doc.add_paragraph(response_dict.get('Authentication Mechanisms', 'No data available'))\n",
    "\n",
    "    doc.add_heading('Integrations', level=2)\n",
    "    doc.add_paragraph(response_dict.get('Integration Points', 'No data available'))\n",
    "\n",
    "    doc.add_heading('Reports and Dashboards', level=2)\n",
    "    doc.add_paragraph(response_dict.get('Reports and Dashboards', 'No data available'))\n",
    "\n",
    "    doc.add_heading('Error Handling', level=2)\n",
    "    doc.add_paragraph(response_dict.get('Error Handling', 'No data available'))\n",
    "\n",
    "    doc.add_heading('Notifications', level=2)\n",
    "    doc.add_paragraph(response_dict.get('Notifications', 'No data available'))\n",
    "\n",
    "    doc.add_heading('Database Objects', level=2)\n",
    "    doc.add_paragraph(response_dict.get('Database Objects', 'No data available'))\n",
    "\n",
    "    doc.add_heading('REST Services and APIs', level=2)\n",
    "    doc.add_paragraph(response_dict.get('REST Services and APIs', 'No data available'))\n",
    "\n",
    "    doc.add_heading('Post-Implementation Support', level=2)\n",
    "    doc.add_paragraph(response_dict.get('Post-Implementation Support', 'No data available'))\n",
    "\n",
    "    # Save the document to a BytesIO object\n",
    "    output = BytesIO()\n",
    "    doc.save(output)\n",
    "    output.seek(0)  # Rewind the buffer for reading\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def upload_to_object_storage(object_storage_client, namespace_name, bucket_name, object_name, file_content):\n",
    "    try:\n",
    "        # Upload the document (in-memory) to OCI Object Storage\n",
    "        print(f\"Uploading {object_name} to bucket {bucket_name}...\")\n",
    "        response = object_storage_client.put_object(\n",
    "            namespace_name,\n",
    "            bucket_name,\n",
    "            object_name,\n",
    "            file_content\n",
    "        )\n",
    "        print(f\"Upload complete: {response.status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload file to Object Storage: {str(e)}\")\n",
    "\n",
    "\n",
    "# Configuration for OCI\n",
    "namespace_name = \"gc35013\"\n",
    "bucket_name = \"ECHO\"\n",
    "object_name = \"PROJECT_SPEAK_MATE/DELIVERY_DOCS/technical_specification.docx\"\n",
    "compartment_id = \"ocid1.compartment.oc1..aaaaaaaaretksgipt3jgwfpzgh4ijyw54uynyfviaxs5li4wtl744fj4fi3q\"\n",
    "\n",
    "# Load OCI configuration from file\n",
    "config = oci.config.from_file(\"config\", \"DEFAULT\")\n",
    "object_storage_client = oci.object_storage.ObjectStorageClient(config)\n",
    "template_path = \"td_template.docx\"\n",
    "\n",
    "# Generate document in memory\n",
    "document_content = generate_technical_spec_template(technical_spec, template_path)\n",
    "\n",
    "# Upload the document to OCI Object Storage\n",
    "upload_to_object_storage(object_storage_client, namespace_name, bucket_name, object_name, document_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
